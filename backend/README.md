# ğŸ§  Backend Service (FastAPI - Python)

## ğŸ” Overview
The backend is the **core logic layer** of the AI Product Recommendation Agent, built with **FastAPI** in Python.  
It handles:
- Chat flow and conversation context.
- Integration with the Large Language Model (LLM).
- Product recommendation logic based on user intent.
- Conversation logging and basic performance monitoring.

---

## ğŸ§© Agent Architecture and Logic Strategy
The system follows a **Hierarchical Two-Step Retrieval** approach â€” optimizing both **token efficiency** and **recommendation accuracy**.

### 1ï¸âƒ£ Initial Flow & Model Setup
- **Provider:** The backend uses the **Groq API**, chosen for its low latency and high throughput â€” crucial for real-time interactions.  
- **Conversation Memory:** The system ensures consistent context across multiple chat turns, maintaining a natural user experience.

### 2ï¸âƒ£ Hierarchical Recommendation Flow
#### Step A: Category Identification *(The Pairing Phase)*
- **Goal:** Identify the most relevant product category based on user intent.  
- **Mapping Logic:**  
  Defined manually in `db/categories_map.py`, this mapping connects user intent â†’ product category â†’ dataset (`db/products.json`).  
  âš ï¸ *If the product dataset changes, this map must be updated to maintain grounding accuracy.*
- **Structured Output:**  
  The model must return:
  ```json
  {
    "Answer": "<short message to the user>",
    "ready_to_filter": <true or false>,
    "selected_category": "<category name or null>"
  }
  ```

The process proceeds to Step B only when a valid category is identified.

#### Step B: Product Selection *(The Grounding Phase)*
- **Contextual Filtering:**
  Only products within the identified category are loaded into the modelâ€™s context.
- **User Needs Summary:**
  Before recommending, the system asks the LLM to summarize the userâ€™s requirements (e.g., â€œneeds powerful GPU, budget under $2000â€).
- **Final Recommendation:**
  The model then selects the best-matching product and generates a natural-language explanation.


## ğŸ—‚ï¸ Directory Structure
```bash
backend/
â”œâ”€â”€ db/
â”‚   â”œâ”€â”€ categories_map.py       # Category mapping logic.
â”‚   â”œâ”€â”€ conversations_log.jsonl # Logs user-agent interactions.
â”‚   â””â”€â”€ products.json           # Product dataset.
â”œâ”€â”€ chat.py                     # Conversation flow controller.
â”œâ”€â”€ data_handler.py             # Data loading and filtering.
â”œâ”€â”€ main.py                     # FastAPI entry point.
â”œâ”€â”€ model.py                    # LLM provider integration.
â”œâ”€â”€ prompts.py                  # System prompts and instructions.
â”œâ”€â”€ .env                        # Environment variables (Groq API key).
â””â”€â”€ requirements.txt            # Python dependencies.
```

## âš™ï¸ Key Technologies & Design
* **Framework:** FastAPI â€” async, fast, and self-documented at `/docs`.
* **LLM Integration:** Modular design for easy provider switching.
* **Validation:** Pydantic models ensure strict schema validation.
* **Grounding:** Product data is loaded from `db/products.json` to anchor recommendations.

## ğŸ“¡ API Endpoints (FastAPI)
| Method | Path | Description | Data Schemas | 
| --- | --- | --- | --- | 
| `POST` | `/chat`| Submits a user message and retrieves the modelâ€™s response & recommendation. | `Message` (in/out) |
| `POST` | `/new_chat` | Starts a new conversation (resets context). | - |
| `POST` | `/feedback` | Submits user feedback for analytics. | `FeedbackData` (in) |
| `GET` | `/logs` | Returns all conversation logs for dashboard or export. | - |

Full API docs available automatically at: `/docs`

## ğŸ“Š Monitoring & Evaluation
* **Logging:** Each turn (user â†” agent) is recorded in `db/conversations_log.jsonl`.
* **Metrics:** Tracks timestamps, latency, number of turns, and feedback results.
* **Export:** `/logs` endpoint provides all stored data for dashboard visualization or offline analysis.

## ğŸš€ Setup & Run Instructions
### Prerequisites:
* Python 3.10+
* `pip` 

### Environment Configuration:
* Create a `.env` file with Groq API key: 
```bash
GROQ_API_KEY=<your_key_from_(https://console.groq.com/keys)>
```
âš ï¸ The .env file should not be pushed to version control.

### â–¶ï¸ Run the Server:
``` bash
cd backend
pip install -r requirements.txt
uvicorn main:app --reload
```
---

âœ… The backend is now ready and fully functional.  
- Backend server: [http://127.0.0.1:8000](http://127.0.0.1:8000)  
- API documentation: [http://127.0.0.1:8000/docs](http://127.0.0.1:8000/docs)
